---
title: "Interpreting and explaining machine learning models"
author: "Computation Skills Workshop"
output: rcfss::xaringan
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  echo = FALSE,
  cache = TRUE,
  message = FALSE,
  warning = FALSE,
  collapse = TRUE,
  fig.retina = 2, fig.width = 12
)

library(tidyverse)
library(tidymodels)
library(ranger)
library(here)
library(DALEX)
library(DALEXtra)
library(lime)
library(patchwork)
library(rcfss)
library(knitr)
library(here)
library(countdown)
library(flipbookr)

set.seed(123)
theme_set(theme_minimal(base_size = rcfss::base_size))
```

# Interpretation

> Interpretability is the degree to which a human can understand the cause of a decision.

--

> Interpretability is the degree to which a human can consistently predict the model's result.

.footnote[
Miller, Tim. "Explanation in artificial intelligence: Insights from the social sciences." arXiv Preprint arXiv:1706.07269. (2017).

Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. "Examples are not enough, learn to criticize! Criticism for interpretability." Advances in Neural Information Processing Systems (2016).
]

---

# Explanation

**Answer to the "why" question**

- Why did the government collapse?
- Why was my loan rejected?
- Why have we not been contacted by alien life yet?

.footnote[Miller, Tim. "Explanation in artificial intelligence: Insights from the social sciences." arXiv Preprint arXiv:1706.07269. (2017).]


--

Focus on specific observations

--

Good explanations are:

- Contrastive
- Selected
- Social
- Truthful
- Generalizable

---

# Global vs. local methods

- Interpretation $\leadsto$ global methods
- Explanation $\leadsto$ local methods

---

# White-box model

Models that lend themselves naturally to interpretation:

- Linear regression
- Logistic regression
- Generalized linear model
- Decision tree

---

# Black-box model

```{r fig.align = 'center', echo = FALSE}
include_graphics(path = "https://imgs.xkcd.com/comics/machine_learning.png")
```

---

# Black-box model

- Random forests
- Boosted trees
- Neural networks
- Deep learning

---

class: inverse

```{r fig.align = "center"}
include_graphics(path = "https://media.giphy.com/media/H8LPekEB8uAFXgw97u/giphy.gif")
```

---

# Predicting student debt

- [College Scorecard](https://collegescorecard.ed.gov/)
- [`rscorecard`](https://github.com/btskinner/rscorecard)

---

# Predicting student debt

```{r get-data, echo = FALSE}
# import data and model
load(here("data", "models.RData"))
```

```{r skim-data, dependson = "get-data", echo = FALSE}
glimpse(scorecard)
```

---

# Construct some models

```{r model-stats, dependson = "get-data", echo = FALSE}
# predict test set for both models and plot
test_preds <- bind_rows(
  `Random forest` = predict(rf_wf, new_data = scorecard_test) %>% bind_cols(scorecard_test),
  `Penalized regression` = predict(glmnet_wf, new_data = scorecard_test) %>% bind_cols(scorecard_test),
  `K nearest neighbors` = predict(kknn_wf, new_data = scorecard_test) %>% bind_cols(scorecard_test),
  .id = "model"
)

# calculate test set RMSE
test_rmse <- test_preds %>%
  group_by(model) %>%
  rmse(truth = debt, estimate = .pred) %>%
  mutate(.estimate = scales::dollar(.estimate, accuracy = 1))

ggplot(data = test_preds, mapping = aes(x = debt, y = .pred)) +
  geom_abline(linetype = 2) +
  geom_point(alpha = 0.25) +
  geom_text(
    mapping = aes(x = 10000, y = 27000, label = glue::glue("RMSE: {.estimate}")),
    data = test_rmse, size = 5
  ) +
  facet_wrap(vars(model)) +
  scale_x_continuous(labels = scales::dollar) +
  scale_y_continuous(labels = scales::dollar) +
  coord_obs_pred() +
  labs(
    x = "Average student debt",
    y = "Predicted student debt"
  ) +
  theme_minimal(base_size = 18)
```

---

class: inverse, center, middle

# Global interpretation methods

---

class: inverse, center, middle

# Permutation-based feature importance

---

# Permutation-based feature importance

* Calculate the increase in the model's prediction error after **permuting** the feature
    * Randomly shuffle the feature's values across observations
* Important feature
* Unimportant feature

--

```
For any given loss function do
1: compute loss function for original model
2: for variable i in {1,...,p} do
     | randomize values
     | apply given ML model
     | estimate loss function
     | compute feature importance (permuted loss / original loss)
   end
3. Sort variables by descending feature importance   
```

---

# Random forest feature importance

```{r explainers, dependons = "get-data"}
explainer_glmnet <- explain_tidymodels(
  model = glmnet_wf,
  data = scorecard_train %>% select(-debt),
  y = scorecard_train$debt,
  label = "penalized regression",
  verbose = FALSE
)

explainer_rf <- explain_tidymodels(
  model = rf_wf,
  data = scorecard_train %>% select(-debt),
  y = scorecard_train$debt,
  label = "random forest",
  verbose = FALSE
)

explainer_kknn <- explain_tidymodels(
  model = kknn_wf,
  data = scorecard_train %>% select(-debt),
  y = scorecard_train$debt,
  label = "k nearest neighbors",
  verbose = FALSE
)
```

```{r vip-rf, dependson = "explainers"}
# random forest model first
vip_rf <- model_parts(explainer_rf, N = NULL)
plot(vip_rf) +
  theme_minimal(base_size = rcfss::base_size) +
  theme(legend.position = "none")
```

---

```{r vip-all, dependson = "explainers"}
# plot variable importance
ggplot_imp <- function(...) {
  obj <- list(...)
  metric_name <- attr(obj[[1]], "loss_name")
  metric_lab <- paste(
    metric_name,
    "after permutations\n(higher indicates more important)"
  )

  full_vip <- bind_rows(obj) %>%
    filter(variable != "_baseline_")

  perm_vals <- full_vip %>%
    filter(variable == "_full_model_") %>%
    group_by(label) %>%
    summarise(dropout_loss = mean(dropout_loss))

  p <- full_vip %>%
    filter(variable != "_full_model_") %>%
    mutate(variable = fct_reorder(variable, dropout_loss)) %>%
    ggplot(aes(dropout_loss, variable))
  if (length(obj) > 1) {
    p <- p +
      facet_wrap(vars(label)) +
      geom_vline(
        data = perm_vals, aes(xintercept = dropout_loss, color = label),
        size = 1.4, lty = 2, alpha = 0.7
      ) +
      geom_boxplot(aes(color = label, fill = label), alpha = 0.2)
  } else {
    p <- p +
      geom_vline(
        data = perm_vals, aes(xintercept = dropout_loss),
        size = 1.4, lty = 2, alpha = 0.7
      ) +
      geom_boxplot(fill = "#91CBD765", alpha = 0.4)
  }
  p +
    theme(legend.position = "none") +
    labs(
      x = metric_lab,
      y = NULL, fill = NULL, color = NULL
    )
}

vip_rf <- model_parts(explainer_rf, N = NULL)
vip_glmnet <- model_parts(explainer_glmnet, N = NULL)
vip_kknn <- model_parts(explainer_kknn, N = NULL)

ggplot_imp(vip_rf, vip_glmnet, vip_kknn)
```

---

class: inverse, center, middle

# Partial dependence plots

---

# Individual conditional expectation

- *Ceteris peribus* - "other things held constant"
- Marginal effect a feature has on the predictor
- Plot one line per observation that shows how the observation's prediction changes when a feature changes
- Partial dependence plot is average of all ICEs


--


```
For a selected predictor (x)
1. Determine grid space of j evenly spaced values across distribution of x
2: for value i in {1,...,j} of grid space do
     | set x to i for all observations
     | apply given ML model
     | estimate predicted value
     | if PDP: average predicted values across all observations
   end
```

---

# Net cost

```{r pdp-netcost, dependson = "explainers"}
# basic pdp for RF model and netcost variable
pdp_netcost <- model_profile(explainer_rf, variables = "netcost", N = 100)

## PDP with ICE curves
plot(pdp_netcost, geom = "profiles")
```

---

# Type

```{r pdp-type, dependson = "explainers"}
# PDP for type
model_profile(explainer_rf, variables = "type", N = NULL) %>%
  plot()
```

---

class: inverse, center, middle

# Interpreting with `DALEX`

---

class: inverse, center, middle

# Local methods

---

class: inverse, center, middle

# Shapley values

```{r local-obs}
uchi <- filter(.data = scorecard, name == "University of Chicago") %>%
  select(-unitid, -name)
wiu <- filter(.data = scorecard, name == "Western Illinois University") %>%
  select(-unitid, -name)
both <- bind_rows(uchi, wiu)
```

---

# University of Chicago

```{r uchi, dependson = "local-obs"}
uchi
```

---

# Breakdown of random forest

```{r bd-rf-1, dependson = c("explainers", "local-obs")}
bd1_rf_distr <- predict_parts(
  explainer = explainer_rf,
  new_observation = uchi,
  type = "break_down",
  order = NULL,
  keep_distributions = TRUE
)
plot(bd1_rf_distr, plot_distributions = TRUE)
```

---

# Breakdown of random forest

```{r bd-rf-2, dependson = c("explainers", "local-obs")}
bd2_rf_distr <- predict_parts(
  explainer = explainer_rf,
  new_observation = uchi,
  type = "break_down",
  order = names(uchi),
  keep_distributions = TRUE
)

plot(bd1_rf_distr, plot_distributions = TRUE) +
  plot(bd2_rf_distr, plot_distributions = TRUE)
```

---

# Breakdown of random forest

```{r bd-rf-random, dependson = c("explainers", "local-obs"), fig.height = 8}
rsample <- map(1:6, function(i) {
  new_order <- sample(1:12)
  bd <- predict_parts(explainer_rf, uchi, order = new_order, type = "break_down")
  bd$variable <- as.character(bd$variable)
  bd$label <- paste("random order no.", i)
  plot(bd)
})
wrap_plots(rsample, ncol = 2)
```

---

# Shapley Additive Explanations (SHAP)

```{r shap-uchi, dependson = c("explainers", "local-obs")}
# explain uchicago with rf model
shap_uchi_rf <- predict_parts(
  explainer = explainer_rf,
  new_observation = uchi,
  type = "shap"
)
plot(shap_uchi_rf)
```

---

# Shapley Additive Explanations (SHAP)

- Average contributions of features are computed under different coalitions of feature orderings
- Randomly permute feature order using $B$ combinations
- Average across individual breakdowns to calculate feature contribution to individual prediction

---

# Shapley Additive Explanations (SHAP)

```{r shap-rf-kknn, dependson = c("explainers", "local-obs")}
# explain uchicago with rf model
shap_uchi_kknn <- predict_parts(
  explainer = explainer_kknn,
  new_observation = uchi,
  type = "shap"
)

plot(shap_uchi_rf) +
  plot(shap_uchi_kknn)
```

---

# Shapley Additive Explanations (SHAP)

```{r shap-both, dependson = c("explainers", "local-obs")}
# explain uchicago with rf model
shap_wiu_rf <- predict_parts(
  explainer = explainer_rf,
  new_observation = wiu,
  type = "shap"
)

{
  plot(shap_uchi_rf) +
    ggtitle("University of Chicago")
} + {
  plot(shap_wiu_rf) +
    ggtitle("Western Illinois University")
}
```

---

class: inverse, center, middle

# LIME

---

# LIME

* Global $\rightarrow$ local
* Interpretable model used to explain individual predictions of a black box model
* Assumes every complex model is linear on a local scale
* Simple model explains the predictions of the complex model **locally**
    * Local fidelity
    * Does not require global fidelity
* Works on tabular, text, and image data

---

# LIME

```{r lime-viz, fig.align = "center", out.width = "78%"}
include_graphics(path = "https://ema.drwhy.ai/figure/lime_introduction.png")
```

.footnote[Source: [*Explanatory Model Analysis*](https://ema.drwhy.ai/LIME.html)]
---

# LIME

1. For each prediction to explain, permute the observation $n$ times
1. Let the complex model predict the outcome of all permuted observations
1. Calculate the distance from all permutations to the original observation
1. Convert the distance to a similarity score
1. Select $m$ features best describing the complex model outcome from the permuted data
1. Fit a simple model to the permuted data, explaining the complex model outcome with the $m$ features from the permuted data weighted by its similarity to the original observation
1. Extract the feature weights from the simple model and use these as explanations for the complex models local behavior

---

# $10$ nearest neighbors

```{r}
# prepare the recipe
prepped_rec_kknn <- extract_recipe(kknn_wf)

# write a function to bake the observation
bake_kknn <- function(x) {
  bake(
    prepped_rec_kknn,
    new_data = x
  )
}

# create explainer object
lime_explainer_kknn <- lime(
  x = scorecard_train,
  model = extract_fit_parsnip(kknn_wf),
  preprocess = bake_kknn
)

# top 5 features
explanation_kknn <- explain(
  x = both,
  explainer = lime_explainer_kknn,
  n_features = 10
)

plot_features(explanation_kknn) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom")
```

---

# Random forest

```{r}
# prepare the recipe
prepped_rec_rf <- extract_recipe(rf_wf)

# write a function to bake the observation
bake_rf <- function(x) {
  bake(
    prepped_rec_rf,
    new_data = x
  )
}

# create explainer object
lime_explainer_rf <- lime(
  x = scorecard_train,
  model = extract_fit_parsnip(rf_wf),
  preprocess = bake_rf
)

# top 5 features
explanation_rf <- explain(
  x = both,
  explainer = lime_explainer_rf,
  n_features = 10
)

plot_features(explanation_rf) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom")
```

---

class: inverse, center, middle

# Explaining with `DALEX` and `lime`
