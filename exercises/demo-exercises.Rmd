---
title: "Interpreting and explaining machine learning models"
output: html_document
---

```{r packages}
# packages for wrangling data and the original models
library(tidyverse)
library(tidymodels)
library(ranger)
library(glmnet)
library(kknn)
library(rcfss)
library(here) # defining consistent filepaths

# packages for model interpretation/explanation
library(DALEX)
library(DALEXtra)

# set random number generator seed value for reproducibility
set.seed(123)
theme_set(theme_minimal())
```

# Import models

```{r}
# load Rdata file with all the data frames and pre-trained models
load(here("data", "models.RData"))

# show pre-trained models
rf_wf
glmnet_wf
kknn_wf
```

## Create explainer objects

```{r}
# use explain_*() to create explainer object
# first step of an DALEX operation
explainer_glmnet <- explain_tidymodels(
  model = glmnet_wf,
  # data should exclude the outcome feature
  data = scorecard_train %>% select(-debt),
  # y should be a vector containing the outcome of interest for the training set
  y = scorecard_train$debt,
  # assign a label to clearly identify model in later plots
  label = "penalized regression"
)

explainer_rf <- explain_tidymodels(
  model = ______,
  data = ______,
  y = ______,
  label = ______
)

explainer_kknn <- explain_tidymodels(
  model = ______,
  data = ______,
  y = ______,
  label = ______
)
```

# Global interpretation methods

## Imputation-based feature importance

```{r}
# random forest model first
vip_rf <- model_parts(explainer_rf)
plot(vip_rf)

# adjust sampling
## N = 100
model_parts(explainer_rf, N = ______) %>%
  plot()

## all observations
model_parts(explainer_rf, N = ______) %>%
  plot()

# calculate ratio rather than raw change
model_parts(explainer_rf, type = ______) %>%
  plot()
```

### Exercises

- Calculate feature importance for the penalized regression and $k$ nearest neighbors using all observations for permutations. How do they compare to the random forest model?
- Calculate feature importance for the random forest model three times, changing the random seed value before each calculation. How do the results change?

```{r}

```

## Partial dependence plots

```{r}
# basic pdp for RF model and netcost variable
#
pdp_netcost <- model_profile(explainer_rf, variables = "netcost")

## just the PDP
plot(pdp_netcost)

## PDP with ICE curves
plot(pdp_netcost, geom = ______)

## larger sample size
model_profile(explainer_rf, variables = "netcost", N = ______) %>%
  plot(geom = ______)

# group by type
pdp_cost_group <- model_profile(explainer_rf, variables = "netcost", groups = ______, N = ______)
plot(pdp_cost_group, geom = ______)
```

### Exercises

- Create PDP + ICE curves for netcost using all three models
- Create a PDP for all numeric variables in the penalized regression model

```{r}

```

- Create a PDP for the state variable and the random forest model

```{r fig.asp = 1.5}

```

# Local explanation methods

## Choose a couple of observations to explain

```{r}
# filter University of Chicago and Western Illinois University from original dataset
uchi <- filter(.data = scorecard, name == "University of Chicago") %>%
  # remove unitid and name variables since they are not used in the model
  select(-unitid, -name)
wiu <- filter(.data = scorecard, name == "Western Illinois University") %>%
  select(-unitid, -name)
```

## Shapley values

```{r}
# explain uchicago with rf model
shap_uchi_rf <- predict_parts(
  explainer = explainer_rf,
  new_observation = uchi,
  type = "shap"
)
plot(shap_uchi_rf)

# explain uchicago with kknn model
shap_uchi_kknn <- predict_parts(
  explainer = ______,
  new_observation = _____,
  type = ______
)
plot(shap_uchi_kknn)

# increase the number of feature order permutations

```

### Pair with `ggplot2`

```{r}
# based on example from https://www.tmwr.org/explain.html#local-explanations

shap_uchi_kknn %>%
  # convert to pure tibble-formatted data frame
  as_tibble() %>%
  # calculate average contribution per variable across permutations
  group_by(variable) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  # reorder variable levels in order of absolute value of mean contribution
  mutate(variable = fct_reorder(variable, abs(mean_val))) %>%
  # define basic ggplot object for horizontal boxplot
  ggplot(mapping = aes(x = contribution, y = variable, fill = mean_val > 0)) +
  # add a bar plot
  geom_col(
    data = ~ distinct(., variable, mean_val),
    mapping = aes(x = mean_val, y = variable),
    alpha = 0.5
  ) +
  # overlay with boxplot to show distribution
  geom_boxplot(width = 0.5) +
  # outcome variable is measured in dollars - contributions are the same units
  scale_x_continuous(labels = scales::dollar) +
  # use viridis color palette
  scale_fill_viridis_d(guide = "none") +
  labs(y = NULL)
```

### Exercises

- Explain each model's prediction for Western Illinois University. How do they differ?

```{r}

```

## LIME

```{r}
# load LIME package - note conflict with DALEX::explain()
library(lime)

# prepare the recipe
prepped_rec_rf <- extract_recipe(rf_wf)

# write a function to bake the observation
bake_rf <- function(x) {
  bake(
    prepped_rec_rf,
    new_data = x
  )
}

# create explainer object
lime_explainer_rf <- lime(
  x = ______,
  model = ______,
  preprocess = ______
)

# top 5 features
explanation_rf <- explain(
  x = ______,
  explainer = ______,
  n_features = ______
)

plot_features(explanation_rf)

# top 10 features, increased permutations
explanation_rf <- explain(
  x = ______,
  explainer = ______,
  n_features = ______,
  n_permutations = ______
)

plot_features(explanation_rf)
```

### Exercises

- Calculate a LIME explanation for Western Illinois and the $k$ nearest neighbors model. What are the top 10 features? How well does the local model explain the prediction?
- Reproduce the explanation but use a lasso model to select the most important features. How does the explanation change?

```{r}

```

## Session Info

```{r}
sessioninfo::session_info()
```
