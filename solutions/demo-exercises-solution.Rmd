---
title: "Interpreting and explaining machine learning models"
output:
  html_document:
    toc: true
    toc_float: true
    theme: cerulean
---

```{r setup, include=FALSE}
options(digits = 3)

knitr::opts_chunk$set(
  collapse = TRUE,
  cache = FALSE,
  message = FALSE,
  warning = FALSE,
  out.width = "90%",
  fig.align = "center",
  fig.width = 8,
  fig.asp = 0.618 # 1 / phi
)
```

```{r packages}
# packages for wrangling data and the original models
library(tidyverse)
library(tidymodels)
library(ranger)
library(glmnet)
library(kknn)
library(rcfss)
library(here) # defining consistent filepaths

# packages for model interpretation/explanation
library(DALEX)
library(DALEXtra)

# set random number generator seed value for reproducibility
set.seed(123)
theme_set(theme_minimal())
```

# Import models

```{r}
# load Rdata file with all the data frames and pre-trained models
load(here("data", "models.Rdata"))

# show pre-trained models
rf_wf
glmnet_wf
kknn_wf
```

## Create explainer objects

```{r}
# use explain_*() to create explainer object
# first step of an DALEX operation
explainer_glmnet <- explain_tidymodels(
  model = glmnet_wf,
  # data should exclude the outcome feature
  data = scorecard_train %>% select(-debt),
  # y should be a vector containing the outcome of interest for the training set
  y = scorecard_train$debt,
  # assign a label to clearly identify model in later plots
  label = "penalized regression"
)

explainer_rf <- explain_tidymodels(
  model = rf_wf,
  data = scorecard_train %>% select(-debt),
  y = scorecard_train$debt,
  label = "random forest"
)

explainer_kknn <- explain_tidymodels(
  model = kknn_wf,
  data = scorecard_train %>% select(-debt),
  y = scorecard_train$debt,
  label = "k nearest neighbors"
)
```

# Global interpretation methods

## Imputation-based feature importance

```{r}
# random forest model first
vip_rf <- model_parts(explainer_rf)
plot(vip_rf)

# adjust sampling
## N = 100
model_parts(explainer_rf, N = 100) %>%
  plot()

## all observations
model_parts(explainer_rf, N = NULL) %>%
  plot()

# calculate ratio rather than raw change
model_parts(explainer_rf, type = "ratio") %>%
  plot()
```

### Exercises

- Calculate feature importance for the penalized regression and $k$ nearest neighbors using all observations for permutations. How do they compare to the random forest model?
- Calculate feature importance for the random forest model three times, changing the random seed value before each calculation. How do the results change?

```{r}
# compare to the glmnet model
vip_glmnet <- model_parts(explainer_glmnet, N = NULL)
plot(vip_glmnet)

# compare to the kknn model
vip_kknn <- model_parts(explainer_kknn, N = NULL)
plot(vip_kknn)

# calculate random forest feature importance thrice
set.seed(123)
model_parts(explainer_rf) %>% plot()

set.seed(234)
model_parts(explainer_rf) %>% plot()

set.seed(345)
model_parts(explainer_rf) %>% plot()
```

## Partial dependence plots

```{r}
# basic pdp for RF model and netcost variable
#
pdp_netcost <- model_profile(explainer_rf, variables = "netcost")

## just the PDP
plot(pdp_netcost)

## PDP with ICE curves
plot(pdp_netcost, geom = "profiles")

## larger sample size
model_profile(explainer_rf, variables = "netcost", N = 500) %>%
  plot(geom = "profiles")

# group by type
pdp_cost_group <- model_profile(explainer_rf, variables = "netcost", groups = "type", N = NULL)
plot(pdp_cost_group, geom = "profiles")
```

### Exercises

- Create PDP + ICE curves for netcost using all three models
- Create a PDP for all numeric variables in the penalized regression model

```{r}
# create PDP + ICE curves for netcost from all three models
model_profile(explainer_rf, variables = "netcost", N = NULL) %>% plot(geom = "profiles")
model_profile(explainer_glmnet, variables = "netcost", N = NULL) %>% plot(geom = "profiles")
model_profile(explainer_kknn, variables = "netcost", N = NULL) %>% plot(geom = "profiles")

# create PDP for all numeric variables in glmnet model
model_profile(explainer_glmnet) %>%
  plot()
```

- Create a PDP for the state variable and the random forest model

```{r fig.asp = 1.5}
# PDP for state
## hard to read
pdp_state_kknn <- model_profile(explainer_kknn, variables = "state", N = NULL)
plot(pdp_state_kknn)

## manually construct and reorder states
## extract aggregated profiles
pdp_state_kknn$agr_profiles %>%
  # convert to tibble
  as_tibble() %>%
  mutate(`_x_` = fct_reorder(.f = `_x_`, .x = `_yhat_`)) %>%
  ggplot(mapping = aes(x = `_yhat_`, y = `_x_`, fill = `_yhat_`)) +
  geom_col() +
  scale_x_continuous(labels = scales::dollar) +
  scale_fill_viridis_c(guide = "none") +
  labs(
    title = "Partial dependence plot for state",
    subtitle = "Created for the k nearest neighbors model",
    x = "Average prediction",
    y = NULL
  )
```

# Local explanation methods

## Choose a couple of observations to explain

```{r}
# filter University of Chicago and Western Illinois University from original dataset
uchi <- filter(.data = scorecard, name == "University of Chicago") %>%
  # remove unitid and name variables since they are not used in the model
  select(-unitid, -name)
wiu <- filter(.data = scorecard, name == "Western Illinois University") %>%
  select(-unitid, -name)
```

## Shapley values

```{r}
# explain uchicago with rf model
shap_uchi_rf <- predict_parts(
  explainer = explainer_rf,
  new_observation = uchi,
  type = "shap"
)
plot(shap_uchi_rf)

# explain uchicago with kknn model
shap_uchi_kknn <- predict_parts(
  explainer = explainer_kknn,
  new_observation = uchi,
  type = "shap"
)
plot(shap_uchi_kknn)

# increase the number of feature order permutations
predict_parts(
  explainer = explainer_kknn,
  new_observation = uchi,
  type = "shap",
  B = 40
) %>%
  plot()
```

### Pair with `ggplot2`

```{r}
# based on example from https://www.tmwr.org/explain.html#local-explanations

shap_uchi_kknn %>%
  # convert to pure tibble-formatted data frame
  as_tibble() %>%
  # calculate average contribution per variable across permutations
  group_by(variable) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  # reorder variable levels in order of absolute value of mean contribution
  mutate(variable = fct_reorder(variable, abs(mean_val))) %>%
  # define basic ggplot object for horizontal boxplot
  ggplot(mapping = aes(x = contribution, y = variable, fill = mean_val > 0)) +
  # add a bar plot
  geom_col(
    data = ~ distinct(., variable, mean_val),
    mapping = aes(x = mean_val, y = variable),
    alpha = 0.5
  ) +
  # overlay with boxplot to show distribution
  geom_boxplot(width = 0.5) +
  # outcome variable is measured in dollars - contributions are the same units
  scale_x_continuous(labels = scales::dollar) +
  # use viridis color palette
  scale_fill_viridis_d(guide = "none") +
  labs(y = NULL)
```

### Exercises

- Explain each model's prediction for Western Illinois University. How do they differ?

```{r}
# calculate shapley values
shap_wiu_rf <- predict_parts(
  explainer = explainer_rf,
  new_observation = wiu,
  type = "shap"
)

shap_wiu_kknn <- predict_parts(
  explainer = explainer_kknn,
  new_observation = wiu,
  type = "shap"
)

shap_wiu_glmnet <- predict_parts(
  explainer = explainer_glmnet,
  new_observation = wiu,
  type = "shap"
)

# generate plots for each
plot(shap_wiu_rf)
plot(shap_wiu_kknn)
plot(shap_wiu_glmnet)

# view side by side
library(patchwork)
plot(shap_wiu_rf) +
  plot(shap_wiu_kknn) +
  plot(shap_wiu_glmnet)

# or combine together and reuse ggplot code from above
bind_rows(
  shap_wiu_rf,
  shap_wiu_kknn,
  shap_wiu_glmnet
) %>%
  # convert to pure tibble-formatted data frame
  as_tibble() %>%
  # calculate average contribution per variable across permutations
  group_by(label, variable) %>%
  mutate(mean_val = mean(contribution)) %>%
  ungroup() %>%
  # reorder variable levels in order of absolute value of mean contribution
  # mutate(variable = fct_reorder(variable, abs(mean_val))) %>%
  mutate(variable = tidytext::reorder_within(x = variable, by = abs(mean_val), within = label)) %>%
  # define basic ggplot object for horizontal boxplot
  ggplot(mapping = aes(x = contribution, y = variable, fill = mean_val > 0)) +
  # add a bar plot
  geom_col(
    data = ~ distinct(., label, variable, mean_val),
    mapping = aes(x = mean_val, y = variable),
    alpha = 0.5
  ) +
  # overlay with boxplot to show distribution
  geom_boxplot(width = 0.5) +
  # facet for each model
  facet_wrap(vars(label), scales = "free_y") +
  tidytext::scale_y_reordered() +
  # outcome variable is measured in dollars - contributions are the same units
  scale_x_continuous(labels = scales::dollar) +
  # use viridis color palette
  scale_fill_viridis_d(guide = "none") +
  labs(y = NULL)
```

## LIME

```{r}
# load LIME package - note conflict with DALEX::explain()
library(lime)

# prepare the recipe
prepped_rec_rf <- extract_recipe(rf_wf)

# write a function to bake the observation
bake_rf <- function(x) {
  bake(
    prepped_rec_rf,
    new_data = x
  )
}

# create explainer object
lime_explainer_rf <- lime(
  x = scorecard_train,
  model = extract_fit_parsnip(rf_wf),
  preprocess = bake_rf
)

# top 5 features
explanation_rf <- explain(
  x = uchi,
  explainer = lime_explainer_rf,
  n_features = 5
)

plot_features(explanation_rf)

# top 10 features, increased permutations
explanation_rf <- explain(
  x = uchi,
  explainer = lime_explainer_rf,
  n_features = 10,
  n_permutations = 2000
)

plot_features(explanation_rf)
```

### Exercises

- Calculate a LIME explanation for Western Illinois and the $k$ nearest neighbors model. What are the top 10 features? How well does the local model explain the prediction?
- Reproduce the explanation but use a lasso model to select the most important features. How does the explanation change?

```{r}
# prepare the recipe
prepped_rec_kknn <- extract_recipe(kknn_wf)

# write a function to bake the observation
bake_kknn <- function(x) {
  bake(
    prepped_rec_kknn,
    new_data = x
  )
}

# create explainer object
lime_explainer_kknn <- lime(
  x = scorecard_train,
  model = extract_fit_parsnip(kknn_wf),
  preprocess = bake_kknn
)

# top 10 features
explanation_kknn <- explain(
  x = uchi,
  explainer = lime_explainer_kknn,
  n_features = 10
)

plot_features(explanation_kknn)

# use lasso to select the most important features
explanation_lasso_kknn <- explain(
  x = uchi,
  explainer = lime_explainer_kknn,
  n_features = 10,
  feature_select = "lasso_path"
)

plot_features(explanation_lasso_kknn)
```

### A note on the penalized regression model

Due to how the model was trained, `bake_glmnet()` requires an additional `composition` argument. Otherwise everything else is the same.

```{r}
# prepare the recipe
prepped_rec_glmnet <- extract_recipe(glmnet_wf)

# write a function to convert the legislative description to an appropriate matrix object
bake_glmnet <- function(x) {
  bake(
    prepped_rec_glmnet,
    new_data = x,
    composition = "dgCMatrix"
  )
}

# create explainer object
lime_explainer_glmnet <- lime(
  x = scorecard_train,
  model = extract_fit_parsnip(glmnet_wf),
  preprocess = bake_glmnet
)

# top 5 features
explanation_glmnet <- explain(
  x = uchi,
  explainer = lime_explainer_glmnet,
  n_features = 5)

plot_features(explanation_glmnet)
```

# Session Info

```{r}
sessioninfo::session_info()
```
